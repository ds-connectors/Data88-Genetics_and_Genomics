{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 88 - Data Science in Genetics and Genomics, 10/29/19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## by Jonathan Fischer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week we talked about principal component analysis (PCA), a ubiquitous statistical technique used in EDA, data visualization, dimension reduction, and data pre-processing. Today we'll build on that and our previous exposure to this method. If you want a refresher, the top answer to this question provides a lovely exposition: https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the modules we'll need\n",
    "from datascience import *\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA finds a good way to summarize your data using fewer dimensions than you started with. The covariance (aka the \"shape\") of your data dictates how easily it can do this. Data with highly correlated features has many redundancies and can thus be represented by fewer dimensions more easily. We can see this directly be considering data with different covariances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on \"spherical\" data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll consider data in which each dimension is statistically independent from the others. This data will have a spherical shape and should be difficult to represent well using fewer dimensions. We'll consider data with 5 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample the means for each dimension. These dimensions are often called features\n",
    "sphere_means = list(np.random.normal(0, 1, 5))\n",
    "\n",
    "# Draw a sample of 5000 points with the appropriate means and covariance\n",
    "x_sphere = np.random.multivariate_normal(sphere_means, np.identity(5), 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We touched briefly on the fact that standardization of our data is often a necessary step of PCA. This helps in a couple of ways. First, it removes the mean which could potentially dominate our signal (see this post https://stats.stackexchange.com/questions/22329/how-does-centering-the-data-get-rid-of-the-intercept-in-regression-and-pca). Subtraction of the mean is called \"centering\" our data. Second, it puts all variables on the same scale such that quantities with naturally higher variances don't dominate (this is \"scaling\"). Depending on your data, the scaling step may or may not be a good idea. It's a bit of an art and knowing what to do will come with experience. For now we'll go ahead and scale. This link (https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html) shows what can happen if we don't scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the sample mean and standard deviation of each feature. \n",
    "# We then use these to standardize our features. If you've seen z-scores before, this should feel familiar.\n",
    "sphere_means = np.mean(x_sphere, axis = 0)\n",
    "sphere_sds = np.std(x_sphere, axis = 0)\n",
    "\n",
    "y_sphere = (x_sphere - sphere_means) / sphere_sds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at what standardization does to our data. \n",
    "# In this case it probably won't do too much since the means are small and the variances were all 1.\n",
    "# Feel free to play around with the means and variances to see whether it plays a difference.\n",
    "\n",
    "# Choose which features to look at (0,1,2,3,4)\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(x_sphere[:,i], x_sphere[:,j], '.')\n",
    "plt.title('Raw data')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(y_sphere[:,i], y_sphere[:,j], '.')\n",
    "plt.title('Standardized data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's perform PCA on our standardized data. \n",
    "# In Python, you first create a PCA object by specifying how many components to estimate.\n",
    "# This number of components cannot exceed the original dimensionality of your data. In fact,\n",
    "# it cannot be greater than min(n_dim, n_obs) for reasons related to linear algebra (rank).\n",
    "pca_sphere = PCA(n_components=5)\n",
    "pca_sphere.fit(y_sphere.T)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The byproduct of PCA are the coordinates of our data in the new set of features. These features are assembled by \n",
    "# taking combinations of our original ones.\n",
    "sphere_comps = pca_sphere.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how our data behave in the new coordinate system. We can also look at the singular values, which represent the strength of each component. You can think of these as the lengths of the axes of an ellipse which enclose our data. By plotting these, we get a sense of how many dimensions are needed to represent our data. Related is the percent of variance explained. This tells us how much of our original signal gets captured by the particular component associated with that singular value. How does the plot look in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(sphere_comps[0,:], sphere_comps[1,:], 'x')\n",
    "plt.title('Projected Data')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(pca.singular_values_)\n",
    "plt.title('Singular values')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.title('Percent variance explained')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature of PCA is that the new variables are linearly independent. You can think of this as meaning these new directions are perpendicular to one another. Mathematically, we can see this by looking at the so-called dot product of any two components; if the value is 0, these two vectors are \"orthogonal\" (perpendicular)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it for any pair below. Also note that the dot product is identical to taking the element-wise product and \n",
    "# then summing the values (up to minor numerical fluctuations)\n",
    "print(np.dot(sphere_comps[0,:], sphere_comps[1,:]), np.sum(sphere_comps[0,:] * sphere_comps[1,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on ellipsoid data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen an example where dimension reduction is hard, let's look at one where there is a fair amount of correlation between the features. You can see this below in the covariance matrix, which has a bit of a block structure. In some sense, this data can be reasonably approximated as two-dimensional despite actually having 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample means with the specified covariance matrix.\n",
    "ellipse_means = list(np.random.normal(0, 1, 5))\n",
    "ellipse_cov = [[1,.8,0,0,0],\n",
    "      [.8,1,0,0,0],\n",
    "      [0,0,1,.8,.8],\n",
    "      [0,0,.8,1,.8],\n",
    "      [0,0,.8,.8,1]]\n",
    "\n",
    "# Draw and standardize your observations. Fill in the correct means and covariance matrix for the elliptical data.\n",
    "x_ellipse = np.random.multivariate_normal(?, ?, 5000) \n",
    "\n",
    "# Compute the mean of each feature and the standard deviation\n",
    "ellipse_means = np.mean(?, axis = ?)\n",
    "ellipse_sds = np.std(?, axis = ?)\n",
    "\n",
    "# Standardize your data\n",
    "y_ellipse = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the raw and standardized data for any two chosen dimensions. Note the shape is much different from before.\n",
    "# Play around with different values of i and j and note what happens.\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(x_ellipse[:,i], x_ellipse[:,j], '.')\n",
    "plt.title('Raw data')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(y_ellipse[:,i], y_ellipse[:,j], '.')\n",
    "plt.title('Standardized data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do PCA on the standardized data. Fill in the correct matrix\n",
    "pca_ellipse = PCA(n_components=5)\n",
    "pca_ellipse.fit(?)  \n",
    "ellipse_comps = pca_ellipse.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce the plots of the projected data, singular values, and variance explained. \n",
    "# Note the similarities and differences between this and the spherical data.\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(ellipse_comps[0,:], ellipse_comps[1,:], 'x')\n",
    "plt.title('Projected Data')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(pca_ellipse.singular_values_)\n",
    "plt.title('Singular values')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(pca_ellipse.explained_variance_ratio_)\n",
    "plt.title('Percent variance explained')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be basically zero again\n",
    "np.dot(ellipse_comps[0,:], ellipse_comps[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that the projections didn't seem very informative in either of the two cases we've considered. This is because all of the data were drawn from the same distribution, so we wouldn't expect PCA to separate any of the points. However, one of the primary uses of PCA is identifying clusters of points that may be lurking in high-dimensional data. To investigate this ability, we'll consider data which contain points from multiple different distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the mean and covariance matrices. Don't worry too much about this step if you're unfamiliar with\n",
    "# covariance matrices (and positive semidefinite matrices). \n",
    "# The important thing to note is that we have 4 clusters of points.\n",
    "\n",
    "means1 = list(np.random.normal(0, 5, 5))\n",
    "means2 = list(np.random.normal(0, 5, 5))\n",
    "means3 = list(np.random.normal(0, 5, 5))\n",
    "means4 = list(np.random.normal(0, 5, 5))\n",
    "\n",
    "cov_mat1 = make_spd_matrix(5, random_state=1)\n",
    "cov_mat2 = make_spd_matrix(5, random_state=2)\n",
    "cov_mat3 = make_spd_matrix(5, random_state=3)\n",
    "cov_mat4 = make_spd_matrix(5, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge our means together and look at a visual representation. \n",
    "# Which features do we expect to well-separate our points?\n",
    "cluster_means = np.concatenate(([means1], [means2], [means3], [means4]))\n",
    "means_heatmap = sns.heatmap(cluster_means)\n",
    "plt.xlabel('Variable')\n",
    "plt.ylabel('Group')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's sample points from each cluster and put them together into a single set of observations.\n",
    "# Fill in with the appropriate means and covariances as suggested by the variable names\n",
    "x1 = np.random.multivariate_normal(?, ?, 1500)\n",
    "x2 = np.random.multivariate_normal(?, ?, 1500)\n",
    "x3 = np.random.multivariate_normal(?, ?, 1500)\n",
    "x4 = np.random.multivariate_normal(?, ?, 1500)\n",
    "\n",
    "x_mixed = np.concatenate((x1, x2, x3, x4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize\n",
    "mixed_means = np.mean(?, axis = 0)\n",
    "mixed_sds = np.std(?, axis = 0)\n",
    "\n",
    "y_mixed = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the raw and standardized data. You might be able to tell that there are 4 groups depending\n",
    "# on your means.\n",
    "\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(x_mixed[:,i], x_mixed[:,j], '.')\n",
    "plt.title('Raw data')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(y_mixed[:,i], y_mixed[:,j], '.')\n",
    "plt.title('Standardized data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca_mixed = PCA(n_components=5)\n",
    "pca_mixed.fit(?)  \n",
    "mixed_comps = pca_mixed.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the projection, singular values, and variance explained. Look at some of the later PCs. There should be\n",
    "# four clusters\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(131)\n",
    "plt.plot(mixed_comps[0,:], mixed_comps[1,:], 'x')\n",
    "plt.title('Projection of Data')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(pca_mixed.singular_values_)\n",
    "plt.title('Singular values')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(pca_mixed.explained_variance_ratio_)\n",
    "plt.title('Proportion of variation explained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(mixed_comps[0,:], mixed_comps[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at a useful tool known as a biplot. This shows the projection of our data and also\n",
    "# draws vectors depicting how our original features are used by our new ones. There might be subtle differences\n",
    "# between the projection here and what we did previously.\n",
    "\n",
    "X = np.concatenate((x1, x2, x3, x4))\n",
    "\n",
    "# code from \n",
    "# https://stackoverflow.com/questions/39216897/plot-pca-loadings-and-loading-in-biplot-in-sklearn-like-rs-autoplot\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X=scaler.transform(X)    \n",
    "pca = PCA()\n",
    "x_new = pca.fit_transform(X)\n",
    "\n",
    "def myplot(score,coeff,labels=None):\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "    n = coeff.shape[0]\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "    plt.scatter(xs * scalex, ys * scaley)\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n",
    "        else:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n",
    "    plt.xlim(-1,1)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.xlabel(\"PC{}\".format(1))\n",
    "    plt.ylabel(\"PC{}\".format(2))\n",
    "    plt.title('PCA Biplot')\n",
    "    plt.grid()\n",
    "    \n",
    "myplot(x_new[:,0:2],np.transpose(pca.components_[0:2, :]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCA to detect batch effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In genetics and genomics, batch effects are one of the biggest problems which sometimes affect our data. PCA is a commonly used to search for these effects, as they typically comprise a dominant source of variation. Here we'll show how batch effects can introduce false signal in real RNA-seq data. These data come from the Geuvadis project and consist of 462 RNA-seq experiments performed on blood samples across 5 different populations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The provided data include reads which have not been corrected for batch, those which have, and the metadata\n",
    "# detailing which lab performed the sequencing as well as the population of each individual.\n",
    "# The data have already been normalized and log-transformed.\n",
    "\n",
    "# Batch effect correction can often be performed using linear models, and that's what I did here. We will\n",
    "# discuss how to do this when we cover that topic in a couple of weeks. \n",
    "\n",
    "# Loading these might take a minute.\n",
    "\n",
    "expression_uncorrected = pd.read_csv('https://raw.githubusercontent.com/ds-connectors/Data88-Genetics_and_Genomics/master/Lab06/geuv_batch.csv').T\n",
    "expression_corrected = pd.read_csv('https://raw.githubusercontent.com/ds-connectors/Data88-Genetics_and_Genomics/master/Lab06/geuv_corrected.csv').T\n",
    "metadata = pd.read_csv('https://raw.githubusercontent.com/ds-connectors/Data88-Genetics_and_Genomics/master/Lab06/geuv_meta.csv').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do PCA on the standardized, uncorrected data\n",
    "uncorrected_means = np.mean(?, axis = 0)\n",
    "uncorrected_sds = np.std(?, axis = 0)\n",
    "\n",
    "y_uncorrected = ?\n",
    "\n",
    "# Let's bump up the number of components since this is a much bigger data set\n",
    "pca_uncorrected = PCA(n_components=100)\n",
    "pca_uncorrected.fit(?)  \n",
    "uncorrected_comps = pca_uncorrected.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare these plots to the analogous ones from the simulated data.\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(pca_uncorrected.singular_values_)\n",
    "plt.title('Singular values')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(pca_uncorrected.explained_variance_ratio_)\n",
    "plt.title('Proportion of variation explained')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at the projections of our samples. We'll color by both batch and population. What do you observe?\n",
    "# Do you think there is some clustering by batch?\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(uncorrected_comps[0,metadata.iloc[2,:]==1], uncorrected_comps[1,metadata.iloc[2,:]==1], 'x', c = 'black')\n",
    "plt.plot(uncorrected_comps[0,metadata.iloc[2,:]==2], uncorrected_comps[1,metadata.iloc[2,:]==2], 'x', c = 'red')\n",
    "plt.plot(uncorrected_comps[0,metadata.iloc[2,:]==3], uncorrected_comps[1,metadata.iloc[2,:]==3], 'x', c = 'yellow')\n",
    "plt.plot(uncorrected_comps[0,metadata.iloc[2,:]==4], uncorrected_comps[1,metadata.iloc[2,:]==4], 'x', c = 'green')\n",
    "plt.plot(uncorrected_comps[0,metadata.iloc[2,:]==5], uncorrected_comps[1,metadata.iloc[2,:]==5], 'x', c = 'cyan')\n",
    "plt.plot(uncorrected_comps[0,metadata.iloc[2,:]==6], uncorrected_comps[1,metadata.iloc[2,:]==6], 'x', c = 'blue')\n",
    "plt.plot(uncorrected_comps[0,metadata.iloc[2,:]==7], uncorrected_comps[1,metadata.iloc[2,:]==7], 'x', c = 'magenta')\n",
    "plt.legend(['1', '2', '3', '4', '5', '6', '7'])\n",
    "plt.title('Projected Data colored by batch')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(uncorrected_comps[0,metadata.iloc[3,:]=='British'], uncorrected_comps[1,metadata.iloc[3,:]=='British'], 'x', c = 'black')\n",
    "plt.plot(uncorrected_comps[0,metadata.iloc[3,:]=='Finland'], uncorrected_comps[1,metadata.iloc[3,:]=='Finland'], 'x', c = 'red')\n",
    "plt.plot(uncorrected_comps[0,metadata.iloc[3,:]=='Tuscan'], uncorrected_comps[1,metadata.iloc[3,:]=='Tuscan'], 'x', c = 'yellow')\n",
    "plt.plot(uncorrected_comps[0,metadata.iloc[3,:]=='Utah'], uncorrected_comps[1,metadata.iloc[3,:]=='Utah'], 'x', c = 'green')\n",
    "plt.plot(uncorrected_comps[0,metadata.iloc[3,:]=='Yoruba'], uncorrected_comps[1,metadata.iloc[3,:]=='Yoruba'], 'x', c = 'cyan')\n",
    "\n",
    "\n",
    "plt.legend(['British', 'Finnish', 'Tuscan', 'Utahn', 'Yoruban'])\n",
    "plt.title('Projected Data colored by population')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do PCA on the standardized, corrected data\n",
    "corrected_means = np.mean(?, axis = 0)\n",
    "corrected_sds = np.std(?, axis = 0)\n",
    "\n",
    "y_corrected = ?\n",
    "\n",
    "pca_corrected = PCA(n_components=100)\n",
    "pca_corrected.fit(?)  \n",
    "corrected_comps = pca_corrected.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(pca_corrected.singular_values_)\n",
    "plt.title('Singular values')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(pca_corrected.explained_variance_ratio_)\n",
    "plt.title('Proportion of variation explained')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now look at the projections of the corrected data, again coloring by batch and population. How does it compare\n",
    "# to what we had before? Do you think gene expression can be used to predict an individual's population? \n",
    "# If you need help, look at how we plotted the uncorrected data and update that appropriately.\n",
    "\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few more notes (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to investigate the effect of certain factors, feel free to play with whether to center and/or scale the data and see what happens. Another interesting quantity to vary is the number of points per cluster. Just repeat the steps we followed but exclude or modify those specific lines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = Notebook('lab6-pca.ok')\n",
    "_ = ok.auth(inline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the assignment.\n",
    "_ = ok.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
