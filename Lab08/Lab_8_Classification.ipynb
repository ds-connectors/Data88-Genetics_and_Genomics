{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Classification \n",
    "## 11/19/19\n",
    "## Due 11/25/19 @ 11:59 PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the modules we'll need\n",
    "from datascience import *\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.manifold import TSNE\n",
    "plt.style.use('fivethirtyeight')\n",
    "from client.api.notebook import Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll explore the task of classification by seeing whether we can successfully predict a cell type given gene expression measurements. To do so, we'll use the single-cell RNA-seq data from last week. These data consist of B cells, T cells, and NK cells from the murine spleen. Let's start by loading and normalizing the data as we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata for single cells\n",
    "sc_meta = pd.read_csv('https://raw.githubusercontent.com/ds-connectors/Data88-Genetics_and_Genomics/master/Lab07/spleen_meta_sc.csv', sep = ',', header = 0).set_index('index').rename(columns ={'Unnamed: 0':'Sample Num'})\n",
    "# Load single-cell expression data and normalize it\n",
    "scRNA_data_pre = pd.read_csv('https://raw.githubusercontent.com/ds-connectors/Data88-Genetics_and_Genomics/master/Lab07/cell_data.csv', sep = ',', header = 0).set_index('Gene')\n",
    "# These data were on log scale so let's go back to counts\n",
    "sc_data = (2 ** scRNA_data_pre - 1)\n",
    "sc_total_med = sc_data.sum().median()\n",
    "sc_data_norm = sc_data / sc_data.sum() * sc_total_med"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is usually a supervised learning task. This means we have data for which we know the true class labels and want to build a model which can predict the label for new data based on what we've already observed. We hence want to get the labels and \"features\" for our data to prepare to build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the cell type labels and gene expression values for all the cells. \n",
    "sc_labels = sc_meta.cell_ontology_class[:]   # The [:] prevents aliasing and generates a new copy\n",
    "sc_features = sc_data_norm.T                 # We need to transpose our data to apply the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key components of model building is splitting our data into training and test sets. We use the training set to construct our model and then evaluate its performance on the test set. There are many ways to split into training and test sets, but our data are amenable to doing so in the most straightforward fashion. Fortunately, there's a handy function that will do everything for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sc_features, sc_labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having split our data, we are now ready to train our model. We're going to use a machine learning algorithm known as a random forest. Go ahead and run the following cell to build the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "clf=RandomForestClassifier(n_estimators = 200)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to compute the error rates on our traning and test sets. We can use the model to make predictions and then compare to what we know to be the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute train and test errors\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = ?  # Predict using the test data\n",
    "\n",
    "[np.mean(y_train_pred == y_train), ?] # Fill in the test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do these errors compare? Why do you think this is?\n",
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is a useful way to visualize the performance of our algorithm. It shows how predictions compare to the truth for each class. Perfect classification would be a diagonal matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce confusion matrix. Fill in the true test labels first and the predicted test labels second.\n",
    "confusion_matrix(?, ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret your confusion matrix. What do you think it means? Does this make sense in light of the t-SNE plots we saw before?\n",
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate plots of our data with the true and predicted labels to see how it does. We'll start by running t-SNE so that we can visualize our cell type clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce t-SNE embeddings\n",
    "x = StandardScaler().fit_transform(np.log2(X_test+1))\n",
    "pca = PCA(n_components = 100)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "tsne = TSNE(n_components = 2)\n",
    "X_embedded = tsne.fit_transform(principalComponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot which colors points by the true labels, predicted labels, and whether the classifier was correct or not.\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.scatter(X_embedded[y_test == 'B cell',0], X_embedded[y_test == 'B cell',1])\n",
    "plt.scatter(X_embedded[y_test == 'T cell',0], X_embedded[y_test == 'T cell',1])\n",
    "plt.scatter(X_embedded[y_test == 'NK cell',0], X_embedded[y_test == 'NK cell',1])\n",
    "plt.legend(['B cells','T cells', 'NK cells'])\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.title('t-SNE, true labels')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.scatter(X_embedded[y_test_pred == 'B cell',0], X_embedded[y_test_pred == 'B cell',1])\n",
    "plt.scatter(X_embedded[y_test_pred == 'T cell',0], X_embedded[y_test_pred == 'T cell',1])\n",
    "plt.scatter(X_embedded[y_test_pred == 'NK cell',0], X_embedded[y_test_pred == 'NK cell',1])\n",
    "plt.legend(['B cells','T cells', 'NK cells'])\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.title('t-SNE, predicted labels')\n",
    "\n",
    "plt.subplot(133)\n",
    "# Fill in the statements such that the plot colors points by whether they were classified correctly or not.\n",
    "# HINT: You may want to use y_test_pred == y_test and/or y_test_pred != y_test\n",
    "plt.scatter(X_embedded[?,0], X_embedded[?,1])\n",
    "plt.scatter(X_embedded[?,0], X_embedded[?,1])\n",
    "plt.legend(['Correct','Incorrect'])\n",
    "plt.xlabel('t-SNE 1')\n",
    "plt.ylabel('t-SNE 2')\n",
    "plt.title('t-SNE, Classification performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests produce a quantity known as feature importances which (hopefully) tell us about how useful certain features are when trying to discriminate among classes. Thus, in theory, features with high importances are relevant for identifying class membership. It's not always so pretty in reality, but it's a good starting point for finding genes which are cell type markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances. Let's look at the top genes\n",
    "feature_imp = pd.Series(clf.feature_importances_,index = X_train.T.index)\n",
    "feature_imp.sort_values(ascending=False)[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the list of genes that have large importances. To my eye, there are three gene prefixes which show up a few times. These are related genes. Identify these three \"families\". Look up a few of the genes online. Do their functions make sense given the task we're performing?\n",
    "\n",
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a histogram of the feature importances. Many of them are zero and the scale is quite compressed, so we'll need to add 1e-10 and then apply np.log10 to get something useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(?) # Fill in the correct quantities to make the requested histogram. HINT: np.log10(? + 1e-10)\n",
    "plt.xlabel('log10 feature importance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Importances for cell type classification')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The manner in which we split our data into training and test sets will affect our model's performance. One factor that affects things is the fraction of data which is training or test. We can change this using the 'test_size' parameter in train_test_split. Let's build a for loop to see how this quantity affects our accuracy rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's re-train with a different class imbalance. How do the confusion matrices compare?\n",
    "test_acc = []\n",
    "for i in [.9, .8, .7, .6, .5, .4, .3, .2, .1]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sc_features, sc_labels, test_size = 1-i)\n",
    "    clf=RandomForestClassifier(n_estimators = 200)\n",
    "    clf.fit(?, ?) # fit the model on the training data\n",
    "    y_test_pred = clf.predict(?) # predict on the test set\n",
    "    test_acc.append(?) # append the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([.9, .8, .7, .6, .5, .4, .3, .2, .1], ?) # Plot the test accuracy as a function of the training fraction\n",
    "plt.xlabel('Fraction of data in training set')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.title('Cell type accuracy vs training set size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment on your resulting plot. What do you think is generally true about the relationship between how much data is in the training set and performance? You may want to run it a few times since there is some randomness in the process.\n",
    "\n",
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection (and model selection) is another crucial component of classification tasks. Essentially, if we have many features, it may not be optimal to use all of them to build the model and we need some way to figure out what to keep and what to discard. One thing we can do is look at the accuracy on the test set as a function of the features we choose to keep. Let's see how our model performs when we truncate at different values of the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's re-train with fewer features (genes). How do the errors compare?\n",
    "X_train, X_test, y_train, y_test = train_test_split(sc_features, sc_labels, test_size=.5)\n",
    "clf=RandomForestClassifier(n_estimators = 200)\n",
    "\n",
    "test_acc = []\n",
    "for i in [0, .000001, .00001, .0001, .001, .01]:\n",
    "    X_train_sub = X_train.T[feature_imp >= i].T\n",
    "    X_test_sub = X_test.T[feature_imp >= i].T\n",
    "\n",
    "    clf.fit(?, ?) # Fit model on the reduced training data\n",
    "    # Compute test accuracy\n",
    "    y_test_pred = clf.predict(?) # Predict on reduced testing data\n",
    "    test_acc.append(?) # Append accuracy at that iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log10([0+1e-10, .000001, .00001, .0001, .001, .01]), ?) # Plot test accuracy\n",
    "plt.xlabel('Log10 feature importance cutoff')\n",
    "plt.ylabel('Test set accuracy')\n",
    "plt.title('Cell type accuracy vs FI cutoff')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment on your resulting plot. For this data, what seems to be true about performance as a function of the number of features we retain? You may want to run it a few times since there is some randomness in the process.\n",
    "\n",
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment on how classification performed compared to clustering using t-SNE. Which was better? What is the key difference that makes one approach preferable to the other?\n",
    "\n",
    "# Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = Notebook('lab08_classification.ok')\n",
    "_ = ok.auth(inline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the assignment.\n",
    "_ = ok.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
